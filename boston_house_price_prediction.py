# -*- coding: utf-8 -*-
"""Boston house price prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Wwy5DZhLr8mmE_IpmGE62pxvtY-QtDR
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings("ignore")

bos = pd.read_csv('/content/housing.csv')
bos.head()

column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
bos = pd.read_csv('/content/housing.csv', header=None, delimiter=r'\s+', names=column_names)
bos.head()

bos.info()

bos.shape

bos.isnull().sum()

bos.isnull()

bos.size

bos.duplicated().sum()

bos.corr()

sns.heatmap(bos.corr())

sns.barplot(x='CHAS', y='MEDV', data=bos)

bos.info()

plt.figure(figsize=(8, 6));
plt.hist(bos['MEDV']);
plt.title('Bostom housing costs and counts histogram');
plt.xlabel('price ($1000s;)');
plt.ylabel('count');
plt.show()

plt.figure(figsize=(8,6));
sns.displot(bos['MEDV']);

X = bos.drop('MEDV', axis=1)

y = bos['MEDV']

stds = StandardScaler()

X = stds.fit_transform(X)

X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.30, random_state=1)

X_train.shape, X_test.shape

## Ml algorithms


from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, mean_squared_error

def grid_search(estimator, id, param_grid):
  grid = GridSearchCV(
      estimator = estimator,
      param_grid = param_grid,
      scoring = {'mse':'neg_mean_squared_error', 'accuracy':'accuracy'},
      refit = 'accuracy',
      cv = 5
  )

  grid.fit(X_train, y_train)

  print(f"{id} BEST ESTIMATOR: {grid.best_estimator_}")
  print(f"{id} BEST PARAMETERS: {grid.best_params_}")
  print(f"{id} BEST SCORE (MSE): {grid.best_score_}")

  y_preds = grid.predict(X_test)
  mse = metrics.mean_squared_error(y_test, y_preds)

  print(f"{id} MSE: {mse}")

from sklearn.linear_model import LinearRegression

lr_model = LinearRegression()

param_grid = {
    'fit_intercept': [True, False],
    'positive':[True, False]
}

grid_search(lr_model, "LinearRegression", param_grid)

from sklearn.ensemble import RandomForestRegressor

reg_model = RandomForestRegressor()

param_grid = {
    'n_estimators': [100,200,300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search(reg_model, "Random Forest", param_grid)

## using neural networks

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=13, activation='relu', input_shape=(13,)),
    tf.keras.layers.Dense(units=10, activation="relu"),
    tf.keras.layers.Dense(units=1, activation='linear')
])

model.compile(optimizer='sgd',
              loss = 'mean_squared_error',
              metrics=['mse'])

model.summary()

## model training

from tensorflow.keras.callbacks import EarlyStopping

cb = EarlyStopping(
    monitor = 'val)loss',
    min_delta = 0.00001,
    patience = 20,
    verbose = 1,
    mode = 'auto',
    baseline = None,
    restore_best_weights = False
)

history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, callbacks=cb)

mse, mae = model.evaluate(X_test, y_test)
print(f"mean squared error is: {mse}")

